{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall llama-cpp-python -y\n",
    "%pip uninstall llama_cpp_python -y\n",
    "%pip uninstall langchain -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export LLAMA_CUBLAS=1\n",
    "!export FORCE_CMAKE=1\n",
    "\n",
    "\n",
    "# !CMAKE_ARGS\"-DLLAMA_METAL=on\"\n",
    "!export CMAKE_ARGS=\"-DLLAMA_CUBLAS=on -DLLAMA_AVX=off -DLLAMA_AVX2=off -DLLAMA_FMA=off\"\n",
    "# !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install llama-cpp-python==0.2.28 --force-reinstall --upgrade --no-cache-dir\n",
    "# %pip install --upgrade --quiet  llama-cpp-python\n",
    "# !CMAKE_ARGS=\"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\" pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q2_K.gguf\n",
    "# !wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q3_K_M.gguf\n",
    "\n",
    "# !wget https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q5_0.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain --force-reinstall --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import pandas as pd\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LlamaCpp(\n",
    "model_path=\"./data/llama-2-7b-chat.Q5_0.gguf\",\n",
    "# model_path=\"./data/phi-2.Q2_K.gguf\",\n",
    "temperature=0,\n",
    "top_p=0,\n",
    "callback_manager=callback_manager,\n",
    "verbose=True,\n",
    "n_gpu_layers=-1,\n",
    "n_ctx=5000,\n",
    "# n_batch = 512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"\"\"[INST]<<SYS>>\n",
    "You're are a helpful Assistant, and you only response to the \"Assistant\"\n",
    "Remember, maintain a natural tone. Be precise, concise, and casual. Keep it short<</SYS>>\n",
    "Hello there how are you?\n",
    "[/INST]\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
